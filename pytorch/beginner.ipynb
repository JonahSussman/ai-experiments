{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html?highlight=module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fda1fbe75b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)  # Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:\n",
      "Parameter containing:\n",
      "tensor([[ 0.2304, -0.1974, -0.0867,  0.2099, -0.4210],\n",
      "        [ 0.2682, -0.0920,  0.2275,  0.0622, -0.0548],\n",
      "        [ 0.1240,  0.0221,  0.1633, -0.1743, -0.0326]], requires_grad=True)\n",
      "------------------\n",
      "bias:\n",
      "Parameter containing:\n",
      "tensor([-0.0403,  0.0648, -0.0018], requires_grad=True)\n",
      "------------------\n",
      "tensor([[ 0.1755, -0.3268, -0.5069],\n",
      "        [-0.6602,  0.2260,  0.1089]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Affine map, function f(x) = Ax + b\n",
    "# - A is a matrix, b is a vector\n",
    "# - A maps from R^m to R^n\n",
    "# - b maps from R^n to R^n\n",
    "lin = nn.Linear(5, 3)  # Maps from R^5 to R^3, parameters A and b\n",
    "\n",
    "print(\"weights:\")\n",
    "print(lin.weight)  # A\n",
    "print(\"------------------\")\n",
    "\n",
    "print(\"bias:\")\n",
    "print(lin.bias)  # b\n",
    "print(\"------------------\")\n",
    "\n",
    "# data is 2x5. A maps from 5 to 3... can we map \"data\" under A?\n",
    "data = torch.randn(2, 5)\n",
    "print(lin(data))  # yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5404, -2.2102],\n",
      "        [ 2.1130, -0.0040]])\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [2.1130, 0.0000]])\n",
      "tensor([[-0.4933, -0.9762],\n",
      "        [ 0.9712, -0.0040]])\n",
      "tensor([[0.3681, 0.0988],\n",
      "        [0.8922, 0.4990]])\n"
     ]
    }
   ],
   "source": [
    "# In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n",
    "# Note that non-linearities typically don't have parameters like affine maps do.\n",
    "# That is, they don't have weights that are updated during training.\n",
    "data = torch.randn(2, 2)\n",
    "print(data)\n",
    "print(F.relu(data))\n",
    "print(F.tanh(data))\n",
    "print(F.sigmoid(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3800, -1.3505,  0.3455,  0.5046,  1.8213])\n",
      "tensor([0.2948, 0.0192, 0.1048, 0.1228, 0.4584])\n",
      "tensor(1.)\n",
      "tensor([-1.2214, -3.9519, -2.2560, -2.0969, -0.7801])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(5)\n",
    "print(data)\n",
    "print(F.softmax(data, dim=0))\n",
    "print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data, dim=0))  # theres also log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'give': 6, 'it': 7, 'to': 8, 'no': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'yo': 23, 'si': 24, 'on': 25}\n",
      "Parameter containing:\n",
      "tensor([[ 0.1194,  0.0609, -0.1268,  0.1274,  0.1191,  0.1739, -0.1099, -0.0323,\n",
      "         -0.0038,  0.0286, -0.1488, -0.1392,  0.1067, -0.0460,  0.0958,  0.0112,\n",
      "          0.0644,  0.0431,  0.0713,  0.0972, -0.1816,  0.0987, -0.1379, -0.1480,\n",
      "          0.0119, -0.0334],\n",
      "        [ 0.1152, -0.1136, -0.1743,  0.1427, -0.0291,  0.1103,  0.0630, -0.1471,\n",
      "          0.0394,  0.0471, -0.1313, -0.0931,  0.0669,  0.0351, -0.0834, -0.0594,\n",
      "          0.1796, -0.0363,  0.1106,  0.0849, -0.1268, -0.1668,  0.1882,  0.0102,\n",
      "          0.1344,  0.0406]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0631, 0.1465], requires_grad=True)\n",
      "tensor([[-0.5378, -0.8771]])\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"me gusta comer en la cafeteria\".lower().split(), \"SPANISH\"),\n",
    "    (\"Give it to me\".lower().split(), \"ENGLISH\"),\n",
    "    (\"No creo que sea una buena idea\".lower().split(), \"SPANISH\"),\n",
    "    (\"No it is not a good idea to get lost at sea\".lower().split(), \"ENGLISH\")\n",
    "]\n",
    "\n",
    "test_data = [\n",
    "    (\"Yo creo que si\".lower().split(), \"SPANISH\"),\n",
    "    (\"it is lost on me\".lower().split(), \"ENGLISH\")\n",
    "]\n",
    "\n",
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its index\n",
    "# into the BOW vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, num_labels: int, vocab_size: int) -> None:\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec: torch.Tensor) -> torch.Tensor:\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "    \n",
    "\n",
    "def make_bow_vector(sentence: list, word_to_ix: dict) -> torch.Tensor:\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    # view(1, -1) is to make it so that it is a row vector \n",
    "    # -1 means \"make the size of this dimension whatever is needed\"\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label: str, label_to_ix: dict) -> torch.Tensor:\n",
    "    return torch.LongTensor([label_to_ix[label]])  # i64\n",
    "\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function of a\n",
    "# module, which was done with the line\n",
    "#\n",
    "# self.linear = nn.Linear(...)\n",
    "# \n",
    "# Then through some Python magic from the PyTorch devs, your module (in this case,\n",
    "# BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "# \n",
    "# You can freeze params (exclude them from training) by setting requires_grad to False\n",
    "# ON THE PARAMETER. \n",
    "# \n",
    "# class MyModule(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.linear1 = nn.Linear(10, 20)\n",
    "#         self.linear2 = nn.Linear(20, 30)\n",
    "#         self.static_param = nn.Parameter(torch.randn(30, 40))\n",
    "#\n",
    "#         # Exclude static_param from training\n",
    "#         self.static_param.requires_grad = False\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "\n",
    "# To run the model, pass in a BoW vector\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    sample = data[0]\n",
    "    bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "    log_probs = model(bow_vector)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's train! To do this, we pass instances through to get log probabilities, compute\n",
    "a loss function, compute the gradient of the loss function, and then update the\n",
    "parameters with a gradient step. Loss functions are provided by Torch in the nn package.\n",
    "nn.NLLLoss() is the negative log likelihood loss we want. It also defines optimization\n",
    "functions in torch.optim. Here, we will just use SGD.\n",
    "\n",
    "Note that the input to NLLLoss is a vector of log probabilities, and a target label. It\n",
    "doesnâ€™t compute the log probabilities for us. This is why the last layer of our network\n",
    "is log softmax. The loss function nn.CrossEntropyLoss() is the same as NLLLoss(), except\n",
    "it does the log softmax for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yo', 'creo', 'que', 'si'] -> tensor([[-0.9297, -0.5020]])\n",
      "['it', 'is', 'lost', 'on', 'me'] -> tensor([[-0.6388, -0.7506]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1194,  0.0609, -0.1268,  0.1274,  0.1191,  0.1739, -0.1099, -0.0323,\n",
      "         -0.0038,  0.0286, -0.1488, -0.1392,  0.1067, -0.0460,  0.0958,  0.0112,\n",
      "          0.0644,  0.0431,  0.0713,  0.0972, -0.1816,  0.0987, -0.1379, -0.1480,\n",
      "          0.0119, -0.0334],\n",
      "        [ 0.1152, -0.1136, -0.1743,  0.1427, -0.0291,  0.1103,  0.0630, -0.1471,\n",
      "          0.0394,  0.0471, -0.1313, -0.0931,  0.0669,  0.0351, -0.0834, -0.0594,\n",
      "          0.1796, -0.0363,  0.1106,  0.0849, -0.1268, -0.1668,  0.1882,  0.0102,\n",
      "          0.1344,  0.0406]], requires_grad=True)\n",
      "tensor([-0.1488, -0.1313], grad_fn=<SelectBackward0>)\n",
      "['yo', 'creo', 'que', 'si'] -> tensor([[-0.2093, -1.6669]])\n",
      "['it', 'is', 'lost', 'on', 'me'] -> tensor([[-2.5330, -0.0828]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0243,  0.4868,  0.2992,  0.5533,  0.5450,  0.5999, -0.6310, -0.8031,\n",
      "         -0.7746,  0.2081,  0.2803,  0.2900,  0.2861,  0.3832,  0.5250,  0.1906,\n",
      "         -0.1853, -0.2066, -0.1784, -0.1525, -0.4314, -0.1510, -0.3876, -0.1480,\n",
      "          0.0119, -0.0334],\n",
      "        [ 0.2103, -0.5395, -0.6003, -0.2832, -0.4550, -0.3156,  0.5841,  0.6237,\n",
      "          0.8102, -0.1323, -0.5605, -0.5222, -0.1126, -0.3941, -0.5126, -0.2388,\n",
      "          0.4294,  0.2134,  0.3603,  0.3347,  0.1230,  0.0829,  0.4380,  0.0102,\n",
      "          0.1344,  0.0406]], requires_grad=True)\n",
      "tensor([ 0.2803, -0.5605], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Run on test data before we train, just to see a before-and-after\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(f\"{instance} -> {log_probs}\")\n",
    "\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters()))\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Step 1. Remember that PyTorch accumulates gradients. We need to clear them out\n",
    "        # before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
    "        # Tensor as an integer. For example, if the target is SPANISH, then\n",
    "        # we wrap the integer 0. The loss function then knows that the 0th\n",
    "        # element of the log probabilities is the log probability\n",
    "        # corresponding to SPANISH\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        target = make_target(label, label_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by calling\n",
    "        # optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(f\"{instance} -> {log_probs}\")\n",
    "\n",
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "print(next(model.parameters()))\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-word2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
