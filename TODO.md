## Readings

### Long term:

- [[Ilya Sutskever's Top 30]]
- [Deep Learning Foundations and Concepts - Bishop](https://www.bishopbook.com/)
- https://d2l.ai/index.html
- Mathematical Statistics with Applications
- https://www.oreilly.com/library/view/natural-language-processing/9781098136789/

### Short term:

- [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- https://jalammar.github.io/illustrated-gpt2/
- [Karpathy `makemore`](https://www.youtube.com/watch?v=PaCmpygFfXo)
- [100+ Statistics Concepts You Should Know](https://)www.youtube.com/watch?v=UhJ_F4uovgE&t=606s
- [The Key Equation Behind Probability](https://www.youtube.com/watch?v=KHVR587oW8I)
-

## Project ideas

- Multi-headed latent attention
  - Add to nanogpt, then GPT2, then granite? (https://www.youtube.com/watch?v=0VLAoVGf_74)
  - GGUF to then run on Ollama or llama.cpp
- Instead of auto-regressive transformer, use diffusion model
